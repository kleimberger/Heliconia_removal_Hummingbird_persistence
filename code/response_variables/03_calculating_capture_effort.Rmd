---
title: "Calculating capture effort (net-hours)"
output: html_document
knit: (function(inputFile, encoding) {
      out_dir <- "knitted_markdown_files";
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_dir=file.path(dirname(inputFile), out_dir))})
---

**Goal**

Calculate capture effort for each capture session, in terms of net-hours.

**Approach**

Net-hour = number of nets x number of hours nets open.

To calculate net-hours, need information about the:

- Number of nets per capture session

  * We used 12-m nets and 6-m  nets; 6-m nets counted as half a net.
  * These data (net lengths) were recorded in the capture notebook, alongside the net GPS coordinates and resources at each net

- Amount of time that nets were open

  * These data (net opening and closing times) were recorded in the capture notebook


```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(ggpubr)
```

## Step 1: Get data and functions

```{r}
#Net locations
nets <- read.csv("../../data/import/Nets_2016-2018_20210909.csv") %>%
   select(-X) %>%
  mutate_all(as.character)%>%
  mutate(date = ymd(date)) %>%
  arrange(date, patch, control_trmt, as.numeric(net_number))

#Net opening/closing times
times <- read.csv("../../data/import/Capture_effort_2016-2018.csv") %>%
  mutate_all(as.character) %>%
  mutate(date = mdy(date))

#Functions
source("../../code/helper_functions/Calculate_basic_summary_stats.R")
```

## Step 2: Calculate length of capture session (in hours)

```{r}
##Calculate capture duration (in hours) based on midpoint of start and end times.##
hours <- times %>%
  mutate(across(c(openStart, openEnd, closeStart, closeEnd), ~hm(.))) %>%
  mutate(openDuration = as.numeric(openEnd - openStart, "hours"), #How long it took to open the nets, in hours
         closeDuration = as.numeric(closeEnd - closeStart,"hours"), #How long it took to close the nets, in hours
         openTime = as.numeric(openStart, "hours") + openDuration/2, #Midpoint 
         closeTime = as.numeric(closeStart, "hours") + closeDuration/2, #Midpoint
         duration = closeTime - openTime)%>% #Duration is in hours
  select(year, patch, control_trmt = controlTreatment, date, open_time = openTime, close_time = closeTime, duration, weather = weatherNotes, data_notes = dataNotes)
```

## Step 3: Calculate number of nets per capture session

```{r}
#Problem: Some net lengths are NA (i.e. missing). For purposes of calculating nets per site (and net hours), replace with NAs with 12m. Since this will be the same pre & post, choice of net length should not matter too much.
missing_lengths <- nets %>%
  distinct(year, patch, control_trmt, date, net_number, net_length) %>%
  filter(is.na(net_length))

nets <- nets %>%
  mutate(net_length = ifelse(is.na(net_length), 12, net_length))

#How many nets of each length were used per capture session?
nets_by_length <- nets %>%
  distinct(year, patch, control_trmt, date, exp_phase, net_number, net_length) %>%
  group_by(year, patch, control_trmt, date, exp_phase, net_length) %>%
  summarise(num_nets = n()) %>%
  ungroup() %>%
  mutate(num_nets_adjusted_for_length = ifelse(net_length == 6, num_nets * 0.5, num_nets * 1)) %>% #6m nets count as half a net
  arrange(year, patch, num_nets, num_nets_adjusted_for_length)

#Add up number of nets per capture session
nets_value_sum <- nets_by_length %>%
  group_by(year, patch, control_trmt, date, exp_phase) %>%
  summarise(net_value = sum(num_nets_adjusted_for_length)) %>%
  ungroup()

#Actual (absolute) number of nets per site
nets_number_sum <- nets_by_length %>%
  group_by(year, patch, control_trmt, date, exp_phase) %>%
  summarise(actual_number_of_nets = sum(num_nets)) %>%
  ungroup()

#Combining into one summary for export
nets_per_site <- nets_number_sum %>%
  left_join(nets_value_sum) %>%
  arrange(year, date)

#How many nets per site, on average?
nets_per_site_sum <- nets_per_site %>%
  calculate_basic_summary_stats(variable = actual_number_of_nets)

nets_per_site_sum
```

## Step 4: Calculate net-hours

Join 'hours' to 'nets_per_site' & calculate net-hours (# nets x # hours)

Adjust net-hours for any nets closed early or opened late

- patch 10 (3/17/2016 capture 2) - Net 2 (12m) closed at 10:00 (instead of 10:40) --> 0.6667 hours less --> subtract 0.6667 net-hours
- patch 24 (3/23/2016 capture 1) - Net 5 (6m) opened at 09:15 (instead of 06:05) --> 3.1667 hours less --> subtract 1.5833 net-hours (because 6m net)
- patch 200 (4/7/2016 capture 1) - Net 6 (6m) opened at 07:10 (instead of 06:00) --> 1.1667 hours less --> subtract 0.5833 net-hours (because 6m net)

```{r}
net_hours <- nets_per_site %>%
  left_join(hours) %>%
  mutate(net_hours = duration * net_value) %>%
  mutate(net_hours_adjusted = ifelse(patch == "10" & year == "2016" & exp_phase == "capture_2", net_hours - 0.6667, net_hours)) %>%
  mutate(net_hours_adjusted = ifelse(patch == "24" & year == "2016" & exp_phase == "capture_1", net_hours - 1.5833, net_hours_adjusted)) %>%
  mutate(net_hours_adjusted = ifelse(patch == "200" & year == "2016" & exp_phase == "capture_1", net_hours - 0.5833, net_hours_adjusted))

#Use adjusted net-hours to calculate capture rates!

#How many net-hours per site, on average?
net_hours_sum <- net_hours %>%
    calculate_basic_summary_stats(variable = net_hours_adjusted)

net_hours_sum

#How many total net-hours, across capture sessions?
net_hours_total <- net_hours %>%
  summarise(net_hours_total = sum(net_hours_adjusted))

net_hours_total
```

## Step 5: Visualize capture effort (pre/post and control/treatment)

Question: when was there a pre-to-post difference in capture effort? This will be important to know for the analysis of recaptures, because that analysis does not control for effort.

```{r}
#Add some variables for plotting
net_hours_to_plot <- net_hours %>%
  mutate(year_patch = paste(year, patch, sep = "_")) %>%
  mutate(control_trmt = factor(control_trmt, levels = c("control", "treatment"), labels = c("Control", "Treatment"))) %>%
  mutate(exp_phase = factor(exp_phase, levels = c("capture_1", "capture_2"), labels = c("Pre", "Post"))) %>%
  mutate(outlier = ifelse(year_patch == "2017_203", "outlier", "not_outlier"))

#Pre-define colors
colors <- c("#0E0D37", "#BA0022")

#Make plot
pp_effort_plot <- net_hours_to_plot %>%
  ggplot(data = ., aes(x = exp_phase, y = net_hours_adjusted)) +
    geom_boxplot(aes(fill = control_trmt), width=0.15, position = position_nudge(x = c(-0.15, 0.15)), alpha=0.5, outlier.shape=NA) +
    stat_summary(fun = mean, position = position_nudge(x = c(-0.15, 0.15)), geom = "point", shape = 18, size = 3) +
    geom_line(aes(group = year_patch, linetype = outlier), alpha = 0.6) +
    geom_point(aes(fill = control_trmt), colour = "black", shape = 21, position = position_dodge(width = 0.45), alpha=1, size=3) + #Non-jittered points
    theme_bw(base_size = 20)+
    scale_fill_manual(values = colors) +
    scale_color_manual(values = colors) +
    theme(legend.position = "none",
          legend.direction = "horizontal",
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          strip.background = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA))+
    labs(x = "", y = "Number of net-hours", fill = "", colour = "") +
    facet_grid(.~control_trmt)

pp_effort_plot

#Does capture effort differ between pre and post within a site?
pre_post_diffs <- net_hours_to_plot %>%
  select(year, patch, control_trmt, exp_phase, net_hours_adjusted) %>%
  pivot_wider(names_from = exp_phase, values_from = net_hours_adjusted) %>%
  mutate(diff = Post - Pre)

#Paired t-test
#Assumption of paired t-test: paired differences are normally distributed
boxplot(filter(pre_post_diffs, control_trmt == "Control")$diff)
boxplot(filter(pre_post_diffs, control_trmt == "Treatment")$diff)
shapiro.test(filter(pre_post_diffs, control_trmt == "Control")$diff)
shapiro.test(filter(pre_post_diffs, control_trmt == "Treatment")$diff)

#Does not look normally distributed
#t.test(net_hours_adjusted ~ exp_phase, data = filter(net_hours_to_plot, control_trmt == "Control"), paired = TRUE)
#t.test(net_hours_adjusted ~ exp_phase, data = filter(net_hours_to_plot, control_trmt == "Treatment"), paired = TRUE)

#Paired Wilcoxon test
#This is alternative to paired t-test
wilcox.test(net_hours_adjusted ~ exp_phase, data = net_hours_to_plot, paired = TRUE)
wilcox.test(net_hours_adjusted ~ exp_phase, data = filter(net_hours_to_plot, control_trmt == "Control"), paired = TRUE)
wilcox.test(net_hours_adjusted ~ exp_phase, data = filter(net_hours_to_plot, control_trmt == "Treatment"), paired = TRUE)
```

Details about Wilcoxon test here: https://data.library.virginia.edu/the-wilcoxon-rank-sum-test/

The impact of ties means the Wilcoxon rank sum distribution cannot be used to calculate exact p-values. If ties occur in our data and we have fewer than 50 observations, the wilcox.test function returns a normal approximated p-value along with a warning message that says “cannot compute exact p-value with ties”.

Whether exact or approximate, p-values do not tell us anything about how different these distributions are. For the Wilcoxon test, a p-value is the probability of getting a test statistic as large or larger assuming both distributions are the same. In addition to a p-value we would like some estimated measure of how these distributions differ. The wilcox.test function provides this information when we set conf.int = TRUE...This returns a “difference in location” measure of -4.65. The documentation for the wilcox.test function states this “does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from x and a sample from y.”

## Step 6: Export

```{r}
#Data
write.csv(net_hours, "../../data/export/intermediate/Net-hours_2016-2018.csv") 

#Figures
ggsave("../../results/response_variables/figures/Capture_effort_pre_post.png", plot = pp_effort_plot, width = 8, height = 6, units = "in", dpi = 300)
```
